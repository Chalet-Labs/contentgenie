# ADR-006: Bulk OPML Import via Trigger.dev Background Task

**Status:** Accepted
**Date:** 2026-02-15
**Issue:** [#36](https://github.com/Chalet-Labs/contentgenie/issues/36)

## Context

Users want to import their podcast subscriptions from other apps via OPML files. OPML (Outline Processor Markup Language) is the de facto standard for podcast subscription interchange. An OPML file can contain dozens to hundreds of feeds, each requiring PodcastIndex lookup (or RSS fallback), podcast upsert, and subscription creation. This is too slow for a synchronous server action or API route.

The existing codebase already has two Trigger.dev patterns for long-running operations:
1. **Fan-out with wait** (`batchSummarizeEpisodes`): Parent task triggers child tasks via `batchTriggerAndWait`, tracks progress via `metadata.set`.
2. **Sequential iteration** (`pollNewEpisodes`): Iterates feeds sequentially with per-feed error isolation via try/catch.

The batch summarize flow also demonstrates real-time progress feedback to the client via `@trigger.dev/react-hooks` (`useRealtimeRun`) with public access tokens generated by `auth.createPublicToken`.

## Options Considered

### Option A: Server action with streaming response

Parse OPML and process each feed in a server action, returning results incrementally.

- **Pro:** No background job infrastructure. Simpler flow.
- **Con:** Server actions don't support streaming. The request would time out for large OPML files (Vercel 10s/60s limits). No retry on failure. No visibility into progress after page navigation.

### Option B: Trigger.dev task with sequential processing (chosen)

A single Trigger.dev task that parses the OPML, iterates feeds sequentially, looks up each feed via PodcastIndex (with RSS fallback for feeds not found), creates subscriptions, and reports progress via `metadata.set`. The client subscribes via `useRealtimeRun`.

- **Pro:** Follows the `pollNewEpisodes` pattern (sequential iteration, per-feed error isolation). 300-second maxDuration handles large imports. Automatic retry on transient failures. Real-time progress via existing `@trigger.dev/react-hooks` pattern. Survives page navigation.
- **Con:** Sequential processing is slower than parallel. Single task run means no per-feed retry isolation via Trigger.dev (handled in-task with try/catch instead).

### Option C: Trigger.dev parent task with per-feed child tasks

A parent task fans out to one child task per feed via `batchTriggerAndWait`.

- **Pro:** Per-feed retry via Trigger.dev. Per-feed visibility in dashboard. Natural isolation.
- **Con:** Overkill for subscription creation (vs. summarization which is genuinely long-running). Higher overhead: N+1 task runs for N feeds. PodcastIndex rate limits could be hit by concurrent child tasks. The `batchSummarizeEpisodes` pattern works because each child task is independently valuable; here, the parent task is the unit of value.

## Decision

**Option B** -- Trigger.dev task with sequential processing.

### Key design decisions

1. **OPML parsing happens in the API route, not the Trigger.dev task.** The file is small (typically <100KB), parsing is fast (<50ms), and validating the file before triggering a background task provides immediate user feedback on malformed files. Only the extracted feed URLs are passed as the task payload.

2. **PodcastIndex lookup first, RSS fallback second.** For each feed URL, attempt to find the podcast via PodcastIndex `searchPodcasts` using the feed URL. If not found, fall back to the existing `addPodcastByRssUrl` logic (parse RSS, create synthetic IDs). This matches the existing dual-source architecture (`source: "podcastindex" | "rss"` in schema).

3. **Deduplication at subscription level.** Before processing, query existing user subscriptions. Skip feeds where the user is already subscribed. This is cheaper than processing each feed and relying on `ON CONFLICT DO NOTHING`.

4. **Progress reporting via `metadata.set`.** Follow the `batchSummarizeEpisodes` pattern: set a `progress` metadata key with `{ total, succeeded, failed, skipped, completed }`. The client reads this via `useRealtimeRun`.

5. **File upload via API route, not server action.** Server actions with file uploads work but have a 4.5MB body limit by default â€” far too generous for OPML files (typically <100KB). An API route gives explicit control: the implementation enforces a **1MB max file size** as a deliberate constraint to prevent resource exhaustion while still accommodating unusually large subscription lists. The API route also provides explicit error responses for validation failures, matching the existing `batch-summarize` pattern.

6. **No schema changes.** The existing `podcasts`, `episodes`, and `user_subscriptions` tables are sufficient. OPML import is a batch subscription creation operation, not a new domain concept.

## Consequences

- A new Trigger.dev task (`import-opml`) is added, increasing the task count and run quota usage.
- The existing `subscribeToPodcast` server action logic (ensure user, ensure podcast, create subscription) is partially duplicated in the Trigger.dev task. This is intentional -- Trigger.dev tasks run on separate infrastructure and cannot call Next.js server actions. The duplication is ~30 lines of DB operations, not worth extracting into a shared module that would need to work in both runtimes (see ADR-003's "No shared dedup helper" precedent).
- OPML files are parsed on the server (API route) and not stored. The extracted feed URLs are passed to Trigger.dev as the task payload. If a task fails, the user must re-upload the file.
- PodcastIndex API rate limits may be stressed by large imports. Sequential processing with built-in delays mitigates this. The existing `retry.onThrow` pattern handles transient failures.
